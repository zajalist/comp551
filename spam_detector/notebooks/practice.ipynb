{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9da7b17",
   "metadata": {},
   "source": [
    "# Phase A: The MLE Approach\n",
    "\n",
    "Here, we use the MLE approach to detect the likelihood of getting a word in a spam or ham message using [the SMS spam dataset collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection).\n",
    "\n",
    "## Limitations\n",
    "Vocabulary Bias: By using .split(), every unique string is treated (including punctuation attached to words like \"win!\") as a unique feature.\n",
    "\n",
    "Bag-of-Words Assumption: This model assumes that the order of words doesn't matter, only their frequency.\n",
    "\n",
    "Zero-multiplier: If a sentence containing many spam words has a single word which is not in our dataset of spam words, because\n",
    "we multiply probabilities, it will render the probability of the whole message \"0\", even though the presence of multiple spam words\n",
    "makes it very likely that it is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e42fdffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 ham words:\n",
      "i: 0.031587\n",
      "you: 0.024172\n",
      "to: 0.022477\n",
      "the: 0.016293\n",
      "a: 0.015323\n",
      "\n",
      "Top 5 spam words:\n",
      "to: 0.038350\n",
      "a: 0.020994\n",
      "call: 0.019147\n",
      "your: 0.014724\n",
      "you: 0.014108\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# Get the path relative to the notebook location\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "data_path = os.path.join('..', 'data', 'sms_spam', 'SMSSpamCollection')\n",
    "\n",
    "# Load the training data\n",
    "df = pd.read_csv(data_path, sep='\\t', header=None, names=['label', 'message'])\n",
    "\n",
    "# Separate spam and ham messages\n",
    "spam_messages = df[df['label'] == 'spam']['message']\n",
    "ham_messages = df[df['label'] == 'ham']['message']\n",
    "\n",
    "# Count word occurrences per category\n",
    "def count_words_by_category(messages):\n",
    "    word_counts = defaultdict(int)\n",
    "    for message in messages:\n",
    "        words = message.lower().split()\n",
    "        for word in words:\n",
    "            word_counts[word] += 1\n",
    "    return word_counts\n",
    "\n",
    "spam_word_counts = count_words_by_category(spam_messages)\n",
    "ham_word_counts = count_words_by_category(ham_messages)\n",
    "\n",
    "# Calculate theta_word for each category\n",
    "total_spam_words = sum(spam_word_counts.values())\n",
    "total_ham_words = sum(ham_word_counts.values())\n",
    "\n",
    "theta_spam = {word: count / total_spam_words for word, count in spam_word_counts.items()}\n",
    "theta_ham = {word: count / total_ham_words for word, count in ham_word_counts.items()}\n",
    "\n",
    "top_n = 5\n",
    "\n",
    "top_ham = sorted(theta_ham.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "top_spam = sorted(theta_spam.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "print(f\"Top {top_n} ham words:\")\n",
    "for word, prob in top_ham:\n",
    "    print(f\"{word}: {prob:.6f}\")\n",
    "\n",
    "print(f\"\\nTop {top_n} spam words:\")\n",
    "for word, prob in top_spam:\n",
    "    print(f\"{word}: {prob:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4536da23",
   "metadata": {},
   "source": [
    "# Phase B: Bayesian Inference (Laplace Smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48aa6e",
   "metadata": {},
   "source": [
    "## Problem with MLE Approach\n",
    "The MLE approach has a critical flaw: **zero-probability trap**. If a word never appears in spam messages, its probability is exactly 0, making any message containing that word have zero probability of being spam (since we multiply probabilities).\n",
    "\n",
    "## Solution: Bayesian Inference with Laplace Smoothing\n",
    "\n",
    "Instead of using raw counts, we add **pseudo-counts** (α, β) representing our prior belief:\n",
    "\n",
    "$$\\hat{\\theta}_{word, Bayesian} = \\frac{\\text{count} + \\alpha}{\\text{total count} + \\alpha \\cdot V}$$\n",
    "\n",
    "Where:\n",
    "- **count**: Number of times the word appears in the category\n",
    "- **α**: Pseudo-count for each word (typically 1)\n",
    "- **V**: Vocabulary size (number of unique words)\n",
    "- **total count + α·V**: Normalization factor\n",
    "\n",
    "This ensures every word has a small but non-zero probability, preventing the zero-probability trap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b19370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13579\n",
      "\n",
      "--- Comparison (MLE vs Bayesian Smoothing) ---\n",
      "\n",
      "Example words that don't appear in one category (would be zero without smoothing):\n",
      "  Word '09099726429' (spam-only): MLE P(word|ham)=0, Bayesian P(word|ham)=0.000012\n",
      "  Word '84128' (spam-only): MLE P(word|ham)=0, Bayesian P(word|ham)=0.000012\n",
      "  Word 'posh' (spam-only): MLE P(word|ham)=0, Bayesian P(word|ham)=0.000012\n",
      "  Word '3230' (ham-only): MLE P(word|spam)=0, Bayesian P(word|spam)=0.000032\n",
      "  Word 'omg' (ham-only): MLE P(word|spam)=0, Bayesian P(word|spam)=0.000032\n",
      "  Word 'jesus..' (ham-only): MLE P(word|spam)=0, Bayesian P(word|spam)=0.000032\n",
      "\n",
      "✓ Notice: Laplace smoothing prevents zero-probability trap!\n",
      "✓ Smallest non-zero probability (Bayesian): 0.0000121027\n"
     ]
    }
   ],
   "source": [
    "# Laplace Smoothing parameters (pseudo-counts)\n",
    "alpha = 1  # Pseudo-count for words\n",
    "beta = 1   # Pseudo-count for vocabulary size\n",
    "\n",
    "# Get vocabulary (all unique words)\n",
    "vocabulary = set(spam_word_counts.keys()) | set(ham_word_counts.keys())\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Implement Bayesian with Laplace Smoothing\n",
    "# Formula: theta_word_bayesian = (count + alpha) / (total_count + alpha * vocab_size)\n",
    "theta_spam_bayes = {}\n",
    "theta_ham_bayes = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    # Spam probabilities with smoothing\n",
    "    spam_count = spam_word_counts.get(word, 0)\n",
    "    theta_spam_bayes[word] = (spam_count + alpha) / (total_spam_words + alpha * vocab_size)\n",
    "    \n",
    "    # Ham probabilities with smoothing\n",
    "    ham_count = ham_word_counts.get(word, 0)\n",
    "    theta_ham_bayes[word] = (ham_count + alpha) / (total_ham_words + alpha * vocab_size)\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"\\n--- Comparison (MLE vs Bayesian Smoothing) ---\")\n",
    "print(\"\\nExample words that don't appear in one category (would be zero without smoothing):\")\n",
    "\n",
    "# Find words that appear in one category but not the other\n",
    "unique_spam_words = set(spam_word_counts.keys()) - set(ham_word_counts.keys())\n",
    "unique_ham_words = set(ham_word_counts.keys()) - set(spam_word_counts.keys())\n",
    "\n",
    "sample_unique = list(unique_spam_words)[:3]\n",
    "for word in sample_unique:\n",
    "    mle_prob = theta_ham.get(word, 0)  # Would be 0 in MLE\n",
    "    bayes_prob = theta_ham_bayes.get(word, 0)\n",
    "    print(f\"  Word '{word}' (spam-only): MLE P(word|ham)={mle_prob}, Bayesian P(word|ham)={bayes_prob:.6f}\")\n",
    "\n",
    "sample_unique = list(unique_ham_words)[:3]\n",
    "for word in sample_unique:\n",
    "    mle_prob = theta_spam.get(word, 0)  # Would be 0 in MLE\n",
    "    bayes_prob = theta_spam_bayes.get(word, 0)\n",
    "    print(f\"  Word '{word}' (ham-only): MLE P(word|spam)={mle_prob}, Bayesian P(word|spam)={bayes_prob:.6f}\")\n",
    "\n",
    "print(\"\\n✓ Notice: Laplace smoothing prevents zero-probability trap!\")\n",
    "print(f\"✓ Smallest non-zero probability (Bayesian): {min(min(theta_spam_bayes.values()), min(theta_ham_bayes.values())):.10f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63bfac",
   "metadata": {},
   "source": [
    "# Phase C: Log-Likelihood & Classification\n",
    "\n",
    "To classify a new sentence, you'll need to sum the log-likelihoods of each word:\n",
    "\n",
    "$$\\log P(\\text{Fake} | \\text{Words}) \\propto \\log P(\\text{Fake}) + \\sum \\log P(\\text{word}_i | \\text{Fake})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0338e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior P(spam) = 0.1341\n",
      "Prior P(ham) = 0.8659\n",
      "\n",
      "--- Classification Results ---\n",
      "\n",
      "Message: \"Congratulations! You won a free prize. Call now!\"\n",
      "  Prediction: SPAM\n",
      "  Log P(spam|message) = -49.92\n",
      "  Log P(ham|message) = -65.27\n",
      "\n",
      "Message: \"Hey, are we still meeting for lunch tomorrow?\"\n",
      "  Prediction: HAM\n",
      "  Log P(spam|message) = -68.21\n",
      "  Log P(ham|message) = -57.04\n",
      "\n",
      "Message: \"URGENT! Claim your lottery winnings now!!!\"\n",
      "  Prediction: SPAM\n",
      "  Log P(spam|message) = -50.11\n",
      "  Log P(ham|message) = -62.05\n",
      "\n",
      "Message: \"Can you pick up milk on your way home?\"\n",
      "  Prediction: HAM\n",
      "  Log P(spam|message) = -71.64\n",
      "  Log P(ham|message) = -60.48\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate prior probabilities\n",
    "total_messages = len(df)\n",
    "prior_spam = len(spam_messages) / total_messages\n",
    "prior_ham = len(ham_messages) / total_messages\n",
    "\n",
    "print(f\"Prior P(spam) = {prior_spam:.4f}\")\n",
    "print(f\"Prior P(ham) = {prior_ham:.4f}\")\n",
    "\n",
    "# Classification function using log-likelihood\n",
    "def classify_message(message, theta_spam, theta_ham, prior_spam, prior_ham, vocab_size, alpha):\n",
    "    \"\"\"\n",
    "    Classify a message as spam or ham using log-likelihood.\n",
    "    \n",
    "    Args:\n",
    "        message: The text message to classify\n",
    "        theta_spam: Dictionary of word probabilities for spam\n",
    "        theta_ham: Dictionary of word probabilities for ham\n",
    "        prior_spam: Prior probability of spam\n",
    "        prior_ham: Prior probability of ham\n",
    "        vocab_size: Size of vocabulary\n",
    "        alpha: Laplace smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (prediction, log_prob_spam, log_prob_ham)\n",
    "    \"\"\"\n",
    "    words = message.lower().split()\n",
    "    \n",
    "    # Start with log priors\n",
    "    log_prob_spam = np.log(prior_spam)\n",
    "    log_prob_ham = np.log(prior_ham)\n",
    "    \n",
    "    # Add log-likelihood for each word\n",
    "    for word in words:\n",
    "        # Get probability for spam (use smoothed probability even for unknown words)\n",
    "        if word in theta_spam:\n",
    "            log_prob_spam += np.log(theta_spam[word])\n",
    "        else:\n",
    "            # Unknown word: use smoothing probability\n",
    "            log_prob_spam += np.log(alpha / (total_spam_words + alpha * vocab_size))\n",
    "        \n",
    "        # Get probability for ham\n",
    "        if word in theta_ham:\n",
    "            log_prob_ham += np.log(theta_ham[word])\n",
    "        else:\n",
    "            # Unknown word: use smoothing probability\n",
    "            log_prob_ham += np.log(alpha / (total_ham_words + alpha * vocab_size))\n",
    "    \n",
    "    # Classify based on which has higher log probability\n",
    "    prediction = \"spam\" if log_prob_spam > log_prob_ham else \"ham\"\n",
    "    \n",
    "    return prediction, log_prob_spam, log_prob_ham\n",
    "\n",
    "# Test the classifier with example messages\n",
    "test_messages = [\n",
    "    \"Congratulations! You won a free prize. Call now!\",\n",
    "    \"Hey, are we still meeting for lunch tomorrow?\",\n",
    "    \"URGENT! Claim your lottery winnings now!!!\",\n",
    "    \"Can you pick up milk on your way home?\"\n",
    "]\n",
    "\n",
    "print(\"\\n--- Classification Results ---\")\n",
    "for msg in test_messages:\n",
    "    pred, log_spam, log_ham = classify_message(\n",
    "        msg, theta_spam_bayes, theta_ham_bayes, \n",
    "        prior_spam, prior_ham, vocab_size, alpha\n",
    "    )\n",
    "    print(f\"\\nMessage: \\\"{msg}\\\"\")\n",
    "    print(f\"  Prediction: {pred.upper()}\")\n",
    "    print(f\"  Log P(spam|message) = {log_spam:.2f}\")\n",
    "    print(f\"  Log P(ham|message) = {log_ham:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
