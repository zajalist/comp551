{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9da7b17",
   "metadata": {},
   "source": [
    "# Phase A: The MLE Approach\n",
    "\n",
    "Here, we use the MLE approach to detect the likelihood of getting a word in a spam or ham message using [the SMS spam dataset collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection).\n",
    "\n",
    "## Limitations\n",
    "Vocabulary Bias: By using .split(), every unique string is treated (including punctuation attached to words like \"win!\") as a unique feature.\n",
    "\n",
    "Bag-of-Words Assumption: This model assumes that the order of words doesn't matter, only their frequency.\n",
    "\n",
    "Zero-multiplier: If a sentence containing many spam words has a single word which is not in our dataset of spam words, because\n",
    "we multiply probabilities, it will render the probability of the whole message \"0\", even though the presence of multiple spam words\n",
    "makes it very likely that it is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42fdffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 ham words:\n",
      "i: 0.031587\n",
      "you: 0.024172\n",
      "to: 0.022477\n",
      "the: 0.016293\n",
      "a: 0.015323\n",
      "\n",
      "Top 5 spam words:\n",
      "to: 0.038350\n",
      "a: 0.020994\n",
      "call: 0.019147\n",
      "your: 0.014724\n",
      "you: 0.014108\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# Get the path relative to the notebook location\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "data_path = os.path.join('..', 'data', 'sms_spam', 'SMSSpamCollection')\n",
    "\n",
    "# Load the training data\n",
    "df = pd.read_csv(data_path, sep='\\t', header=None, names=['label', 'message'])\n",
    "\n",
    "# Separate spam and ham messages\n",
    "spam_messages = df[df['label'] == 'spam']['message']\n",
    "ham_messages = df[df['label'] == 'ham']['message']\n",
    "\n",
    "# Count word occurrences per category\n",
    "def count_words_by_category(messages):\n",
    "    word_counts = defaultdict(int)\n",
    "    for message in messages:\n",
    "        words = message.lower().split()\n",
    "        for word in words:\n",
    "            word_counts[word] += 1\n",
    "    return word_counts\n",
    "\n",
    "spam_word_counts = count_words_by_category(spam_messages)\n",
    "ham_word_counts = count_words_by_category(ham_messages)\n",
    "\n",
    "# Calculate theta_word for each category\n",
    "total_spam_words = sum(spam_word_counts.values())\n",
    "total_ham_words = sum(ham_word_counts.values())\n",
    "\n",
    "theta_spam = {word: count / total_spam_words for word, count in spam_word_counts.items()}\n",
    "theta_ham = {word: count / total_ham_words for word, count in ham_word_counts.items()}\n",
    "\n",
    "top_n = 5\n",
    "\n",
    "top_ham = sorted(theta_ham.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "top_spam = sorted(theta_spam.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "print(f\"Top {top_n} ham words:\")\n",
    "for word, prob in top_ham:\n",
    "    print(f\"{word}: {prob:.6f}\")\n",
    "\n",
    "print(f\"\\nTop {top_n} spam words:\")\n",
    "for word, prob in top_spam:\n",
    "    print(f\"{word}: {prob:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4536da23",
   "metadata": {},
   "source": [
    "# Phase B: Bayesian Inference (Laplace Smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48aa6e",
   "metadata": {},
   "source": [
    "## Problem with MLE Approach\n",
    "The MLE approach has a critical flaw: **zero-probability trap**. If a word never appears in spam messages, its probability is exactly 0, making any message containing that word have zero probability of being spam (since we multiply probabilities).\n",
    "\n",
    "## Solution: Bayesian Inference with Laplace Smoothing\n",
    "\n",
    "Instead of using raw counts, we add **pseudo-counts** (α, β) representing our prior belief:\n",
    "\n",
    "$$\\hat{\\theta}_{word, Bayesian} = \\frac{\\text{count} + \\alpha}{\\text{total count} + \\alpha \\cdot V}$$\n",
    "\n",
    "Where:\n",
    "- **count**: Number of times the word appears in the category\n",
    "- **α**: Pseudo-count for each word (typically 1)\n",
    "- **V**: Vocabulary size (number of unique words)\n",
    "- **total count + α·V**: Normalization factor\n",
    "\n",
    "This ensures every word has a small but non-zero probability, preventing the zero-probability trap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b19370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13579\n",
      "\n",
      "--- Comparison (MLE vs Bayesian Smoothing) ---\n",
      "\n",
      "Example words that don't appear in one category (would be zero without smoothing):\n",
      "  Word '87077' (spam-only): MLE P(word|ham)=0, Bayesian P(word|ham)=0.000012\n",
      "  Word '80878.' (spam-only): MLE P(word|ham)=0, Bayesian P(word|ham)=0.000012\n",
      "  Word '2yr' (spam-only): MLE P(word|ham)=0, Bayesian P(word|ham)=0.000012\n",
      "  Word 'mo' (ham-only): MLE P(word|spam)=0, Bayesian P(word|spam)=0.000032\n",
      "  Word 'ystrday.ice' (ham-only): MLE P(word|spam)=0, Bayesian P(word|spam)=0.000032\n",
      "  Word 'loose' (ham-only): MLE P(word|spam)=0, Bayesian P(word|spam)=0.000032\n",
      "\n",
      "✓ Notice: Laplace smoothing prevents zero-probability trap!\n",
      "✓ Smallest non-zero probability (Bayesian): 0.0000121027\n"
     ]
    }
   ],
   "source": [
    "# Laplace Smoothing parameters (pseudo-counts)\n",
    "alpha = 1  # Pseudo-count for words\n",
    "beta = 1   # Pseudo-count for vocabulary size\n",
    "\n",
    "# Get vocabulary (all unique words)\n",
    "vocabulary = set(spam_word_counts.keys()) | set(ham_word_counts.keys())\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Implement Bayesian with Laplace Smoothing\n",
    "# Formula: theta_word_bayesian = (count + alpha) / (total_count + alpha * vocab_size)\n",
    "theta_spam_bayes = {}\n",
    "theta_ham_bayes = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    # Spam probabilities with smoothing\n",
    "    spam_count = spam_word_counts.get(word, 0)\n",
    "    theta_spam_bayes[word] = (spam_count + alpha) / (total_spam_words + alpha * vocab_size)\n",
    "    \n",
    "    # Ham probabilities with smoothing\n",
    "    ham_count = ham_word_counts.get(word, 0)\n",
    "    theta_ham_bayes[word] = (ham_count + alpha) / (total_ham_words + alpha * vocab_size)\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"\\n--- Comparison (MLE vs Bayesian Smoothing) ---\")\n",
    "print(\"\\nExample words that don't appear in one category (would be zero without smoothing):\")\n",
    "\n",
    "# Find words that appear in one category but not the other\n",
    "unique_spam_words = set(spam_word_counts.keys()) - set(ham_word_counts.keys())\n",
    "unique_ham_words = set(ham_word_counts.keys()) - set(spam_word_counts.keys())\n",
    "\n",
    "sample_unique = list(unique_spam_words)[:3]\n",
    "for word in sample_unique:\n",
    "    mle_prob = theta_ham.get(word, 0)  # Would be 0 in MLE\n",
    "    bayes_prob = theta_ham_bayes.get(word, 0)\n",
    "    print(f\"  Word '{word}' (spam-only): MLE P(word|ham)={mle_prob}, Bayesian P(word|ham)={bayes_prob:.6f}\")\n",
    "\n",
    "sample_unique = list(unique_ham_words)[:3]\n",
    "for word in sample_unique:\n",
    "    mle_prob = theta_spam.get(word, 0)  # Would be 0 in MLE\n",
    "    bayes_prob = theta_spam_bayes.get(word, 0)\n",
    "    print(f\"  Word '{word}' (ham-only): MLE P(word|spam)={mle_prob}, Bayesian P(word|spam)={bayes_prob:.6f}\")\n",
    "\n",
    "print(\"\\n✓ Notice: Laplace smoothing prevents zero-probability trap!\")\n",
    "print(f\"✓ Smallest non-zero probability (Bayesian): {min(min(theta_spam_bayes.values()), min(theta_ham_bayes.values())):.10f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
